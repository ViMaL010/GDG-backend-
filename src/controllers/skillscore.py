# -*- coding: utf-8 -*-
"""skillscore

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gUD2EhLCT5kvKYJY6cpRo2X25E36r9uV
"""

import requests
import pandas as pd
import time
from functools import lru_cache
import os
from datetime import datetime, timedelta
import json
import subprocess
import shutil
import concurrent.futures
from collections import defaultdict
import tempfile
import logging
from pathlib import Path



# GitHub Authentication - More secure approach
def get_github_token():
    """Get GitHub token from environment variable."""
    token = os.environ.get("GITHUB_TOKEN", "")
    if not token:
        print("Warning: GITHUB_TOKEN environment variable not set. API rate limits will be lower.")
        return ""
    return token

def get_headers():
    """Get headers for GitHub API requests with token if available."""
    token = get_github_token()
    if token:
        return {"Authorization": f"token {token}"}
    return {}

@lru_cache(maxsize=128)
def make_api_request(url, retry_count=3, retry_delay=1):
    """
    Cached API request function with retry logic.
    
    Args:
        url: API endpoint URL
        retry_count: Number of retries on failure
        retry_delay: Delay between retries in seconds
    
    Returns:
        JSON response or None on failure
    """
    headers = get_headers()
    
    for attempt in range(retry_count):
        try:
            response = requests.get(url, headers=headers, timeout=30)
            
            # Check for rate limiting
            if response.status_code == 403 and 'X-RateLimit-Remaining' in response.headers:
                if int(response.headers['X-RateLimit-Remaining']) == 0:
                    reset_time = int(response.headers['X-RateLimit-Reset'])
                    sleep_time = max(1, reset_time - int(time.time()) + 1)
                    print(f"Rate limit exceeded. Waiting for {sleep_time} seconds...")
                    time.sleep(sleep_time)
                    continue
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 404:
                #print(f"Resource not found: {url}")
                return None
            else:
                print(f"API request failed with status {response.status_code}: {url}")
                
                # If we have attempts left, retry after delay
                if attempt < retry_count - 1:
                    time.sleep(retry_delay * (attempt + 1))  # Exponential backoff
                    continue
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"Request error: {e}")
            if attempt < retry_count - 1:
                time.sleep(retry_delay * (attempt + 1))
            else:
                return None
    
    return None

def paginated_request(base_url, params=None, max_pages=10):
    """
    Generic function for paginated GitHub API requests with page limit.
    
    Args:
        base_url: Base API URL
        params: Additional query parameters
        max_pages: Maximum number of pages to fetch
    
    Returns:
        List of items from all pages
    """
    items = []
    page = 1
    
    while page <= max_pages:
        url = f"{base_url}{'&' if '?' in base_url else '?'}page={page}&per_page=100"
        if params:
            for key, value in params.items():
                url += f"&{key}={value}"

        data = make_api_request(url)
        if not data or (isinstance(data, list) and not data) or (isinstance(data, dict) and not data.get("items")):
            break

        new_items = data if isinstance(data, list) else data.get("items", [])
        items.extend(new_items)
        
        if len(new_items) < 100:
            break

        page += 1
        time.sleep(0.7)  # Slightly increased sleep time to avoid rate limiting

    return items

def calculate_longest_streak(events):
    """
    Calculate the longest contribution streak from GitHub activity events.
    
    Args:
        events: List of GitHub events
    
    Returns:
        Integer representing longest consecutive days with contributions
    """
    if not events:
        return 0

    try:
        event_dates = sorted(set(
            datetime.strptime(event.get("created_at", "")[:10], "%Y-%m-%d")
            for event in events if event.get("created_at")
        ))

        if not event_dates:
            return 0

        longest_streak = current_streak = 1
        for i in range(1, len(event_dates)):
            if (event_dates[i] - event_dates[i - 1]).days <= 1:
                current_streak += 1
                longest_streak = max(longest_streak, current_streak)
            else:
                current_streak = 1

        return longest_streak
    except Exception as e:
        print(f"Error calculating streak: {e}")
        return 0

def fetch_github_data(username):
    """
    Fetch GitHub profile data for a single user with optimized API calls.
    
    Args:
        username: GitHub username
    
    Returns:
        List containing user data or empty list on failure
    """
    data = []

    # Contribution types for counting events
    contribution_types = {
        "PushEvent", "PullRequestEvent", "PullRequestReviewEvent", "IssueCommentEvent",
        "IssuesEvent", "CreateEvent", "DiscussionEvent", "DiscussionCommentEvent",
        "GollumEvent", "ForkEvent", "SponsorshipEvent", "PublicEvent", "MemberEvent"
    }

    try:
        #print(f"Fetching data for {username}...")

        # Fetch user profile data
        user_data = make_api_request(f"https://api.github.com/users/{username}")
        if not user_data:
            print(f"Error: Could not fetch basic profile data for {username}")
            return data

        name = user_data.get("name", "")
        joining_year = user_data.get("created_at", "")[:4]
        repositories = user_data.get("public_repos", 0)
        followers = user_data.get("followers", 0)

        # Fetch starred repos (limit to 1 page for optimization)
        starred = paginated_request(f"https://api.github.com/users/{username}/starred", {"per_page": 100}, max_pages=1)
        stars = len(starred)

        # Fetch events with a reasonable limit
        all_events = paginated_request(f"https://api.github.com/users/{username}/events", max_pages=3)
        longest_streak = calculate_longest_streak(all_events)

        # Count contributions by type
        contribution_count = sum(1 for event in all_events if event.get("type") in contribution_types)

        # Fetch repositories data
        repos_data = paginated_request(f"https://api.github.com/users/{username}/repos", {"type": "all"}, max_pages=5)

        # Initialize counters
        total_commits = 0
        total_forks = 0
        total_issues = 0
        total_pr_created = 0
        total_pr_merged = 0
        total_collaborations = 0
        github_actions_used = 0
        documentation_score = 0

        # Set a limit for repositories to analyze (to avoid excessive API calls)
        repo_limit = min(10, len(repos_data))
        analyzed_repos = repos_data[:repo_limit]
        
        #print(f"Analyzing {repo_limit} out of {len(repos_data)} repositories...")

        # Process repositories in batch with limits to avoid rate limiting
        for repo in analyzed_repos:
            repo_name = repo.get("name", "")
            repo_full_name = repo.get("full_name", "")

            # Accumulate fork counts
            total_forks += repo.get("forks_count", 0)

            # Check if README exists (documentation score)
            readme = make_api_request(f"https://api.github.com/repos/{repo_full_name}/readme")
            if readme:
                documentation_score += 1

            # Count commit stats for this repo (limit to newest 100)
            commits = paginated_request(
                f"https://api.github.com/repos/{repo_full_name}/commits", 
                {"author": username}, 
                max_pages=1
            )
            total_commits += len(commits)

            # Get issues, PRs, and collaborators with limits
            issues = paginated_request(
                f"https://api.github.com/repos/{repo_full_name}/issues",
                {"state": "all"},
                max_pages=1
            )
            total_issues += len(issues)

            pulls = paginated_request(
                f"https://api.github.com/repos/{repo_full_name}/pulls", 
                {"state": "all"}, 
                max_pages=1
            )
            total_pr_created += len(pulls)
            total_pr_merged += sum(1 for pr in pulls if pr.get("merged_at"))

            # Only check collaborators if user has push access (avoid 403 errors)
            if repo.get("permissions", {}).get("push", False):
                collaborators = paginated_request(
                    f"https://api.github.com/repos/{repo_full_name}/collaborators",
                    max_pages=1
                )
                total_collaborations += len(collaborators)

            # Check for GitHub Actions
            workflows = make_api_request(f"https://api.github.com/repos/{repo_full_name}/actions/workflows")
            if workflows and workflows.get("total_count", 0) > 0:
                github_actions_used += 1

            # Sleep to avoid rate limiting
            time.sleep(0.5)

        # Calculate documentation score (avoid division by zero)
        documentation_score = round(documentation_score / max(1, min(repositories, repo_limit)), 2)

        # Fetch gists with limit
        gists = paginated_request(f"https://api.github.com/users/{username}/gists", max_pages=1)
        gist_count = len(gists)

        # Fetch merged PRs with limit
        merged_prs = paginated_request(
            f"https://api.github.com/search/issues?q=author:{username}+type:pr+is:merged",
            max_pages=1
        )
        accepted_prs = len(merged_prs)

        # Calculate total contributions
        total_contributions = contribution_count + total_commits + gist_count + accepted_prs

        # Append data
        data.append([
            username, name, joining_year, repositories, total_contributions, total_commits, stars,
            followers, total_collaborations, documentation_score, total_forks, total_issues,
            total_pr_created, total_pr_merged, github_actions_used, longest_streak
        ])

        #print(f"Completed data collection for {username}")

    except Exception as e:
        print(f"Error processing {username}: {e}")
        # Return empty data on error
        return []

    return data

def calculate_github_score(data):
    """
    Calculate GitHub profile scores with an improved algorithm.
    
    Args:
        data: List of GitHub user data
    
    Returns:
        List of scores
    """
    if not data:
        return []

    # Define scoring categories and their weights
    categories = {
        "activity": {
            "total_contributions": 10,
            "repositories": 5,
            "longest_streak": 6,
        },
        "impact": {
            "stars": 8,
            "total_forks": 7,
            "followers": 7,
        },
        "quality": {
            "documentation_score": 6,
            "github_actions_used": 6,
            "pr_success_rate": 7,
        },
        "collaboration": {
            "total_collaborations": 6,
            "total_issues": 5,
            "total_pr_created": 6,
            "total_pr_merged": 7,
        }
    }

    # Flatten weights for calculation
    weights = {}
    for category, metrics in categories.items():
        weights.update(metrics)

    scores = []

    for row in data:
        # Create a dictionary from the row for easier access
        user_data = {
            "username": row[0],
            "name": row[1],
            "joining_year": row[2],
            "repositories": row[3],
            "total_contributions": row[4],
            "total_commits": row[5],
            "stars": row[6],
            "followers": row[7],
            "total_collaborations": row[8],
            "documentation_score": row[9],
            "total_forks": row[10],
            "total_issues": row[11],
            "total_pr_created": row[12],
            "total_pr_merged": row[13],
            "github_actions_used": row[14],
            "longest_streak": row[15]
        }

        # Calculate account age in years (using current year 2025)
        try:
            joining_year = int(user_data["joining_year"])
            account_age = max(1, 2025 - joining_year)  # Use at least 1 year to avoid division by zero
        except (ValueError, TypeError):
            account_age = 1  # Default if joining_year is invalid

        # Calculate PR success rate (with a minimum threshold of 5 PRs)
        pr_created = max(1, user_data["total_pr_created"])  # Avoid division by zero
        pr_success_rate = min(1.0, user_data["total_pr_merged"] / pr_created) if pr_created >= 5 else 0.5

        # Add time-normalized metrics
        normalized_data = {
            # Original metrics
            **user_data,
            # Add PR success rate
            "pr_success_rate": pr_success_rate,
            # Time-normalized metrics (per year)
            "repositories_per_year": user_data["repositories"] / account_age,
            "contributions_per_year": user_data["total_contributions"] / account_age,
            "commits_per_year": user_data["total_commits"] / account_age,
        }

        # Calculate category scores
        category_scores = {}

        # Define base values for normalization (representing "good" values)
        base_values = {
            "repositories": 20,
            "total_contributions": 300,
            "longest_streak": 30,
            "stars": 50,
            "total_forks": 20,
            "followers": 50,
            "documentation_score": 0.5,
            "github_actions_used": 3,
            "pr_success_rate": 0.7,
            "total_collaborations": 5,
            "total_issues": 20,
            "total_pr_created": 50,
            "total_pr_merged": 40,
        }

        # Sigmoid normalization function to create S-curve scoring (values in 0-1 range)
        def sigmoid_normalize(value, base):
            return 2 / (1 + 2.71828 ** (-value / base)) - 1

        # Calculate score for each category
        for category, metrics in categories.items():
            category_score = 0
            max_category_score = 0

            for metric, weight in metrics.items():
                if metric == "pr_success_rate":
                    # PR success rate is already normalized between 0-1
                    normalized_value = normalized_data["pr_success_rate"]
                else:
                    # Apply sigmoid normalization
                    normalized_value = sigmoid_normalize(normalized_data[metric], base_values[metric])

                metric_score = normalized_value * weight
                category_score += metric_score
                max_category_score += weight

            # Normalize category score to 100 points scale
            category_scores[category] = (category_score / max_category_score) * 100

        # Calculate overall score (average of category scores)
        overall_score = round(sum(category_scores.values()) / len(category_scores), 2)
        scores.append(overall_score)

    return scores

def run_github_analysis(username):
    """
    Run the GitHub analysis for a single username and return a DataFrame.
    
    Args:
        username: GitHub username
    
    Returns:
        DataFrame with GitHub analysis results or None on failure
    """
    # Validate username
    if not username or not isinstance(username, str):
        print("Error: Invalid GitHub username")
        return None

    # Fetch data and calculate scores
    github_data = fetch_github_data(username)
    if not github_data:
        print(f"No data fetched for GitHub user: {username}")
        return None

    scores = calculate_github_score(github_data)
    if not scores:
        print(f"Could not calculate GitHub score for: {username}")
        return None

    # Define columns for the DataFrame
    columns = [
        "Username", "Name", "Joining Year", "Repositories", "Total Contributions",
        "Total Commits", "Stars", "Followers", "Total Collaborations",
        "Documentation Score", "Total Forks", "Total Issues",
        "Total PR Created", "Total PR Merged", "GitHub Actions Used",
        "Longest Streak", "GitHub Score"
    ]

    # Create a DataFrame with the new data
    try:
        df = pd.DataFrame([github_data[0] + [scores[0]]], columns=columns)
    except Exception as e:
        print(f"Error creating DataFrame: {e}")
        return None

    # Reorganize columns for better readability
    column_order = [
        "Username", "Name", "Joining Year", "Repositories", "Total Contributions",
        "Total Commits", "Stars", "Followers", "Total Collaborations",
        "Documentation Score", "Total Forks", "Total Issues",
        "Total PR Created", "Total PR Merged", "GitHub Actions Used",
        "Longest Streak", "GitHub Score"
    ]

    # Only include columns that exist in the DataFrame
    ordered_columns = [col for col in column_order if col in df.columns]
    df = df[ordered_columns]

    # Save data to CSV
    try:
        save_github_data_to_csv(df, "github_profiles.csv")
    except Exception as e:
        print(f"Warning: Failed to save to CSV: {e}")
    
    return df

def save_github_data_to_csv(df, csv_path="github_profiles.csv"):
    """
    Save GitHub data to CSV file with proper handling of file operations.
    
    Args:
        df: DataFrame to save
        csv_path: Path to CSV file
    """
    if df is None or df.empty:
        print("No data to save")
        return

    try:
        # Check if file exists
        file_exists = os.path.isfile(csv_path)
        
        if file_exists:
            # Read existing data
            try:
                existing_df = pd.read_csv(csv_path)
                
                # Ensure consistent column names
                existing_df.columns = existing_df.columns.str.strip()
                df.columns = df.columns.str.strip()
                
                # Check if the username already exists
                new_username = df.iloc[0]['Username']
                
                # Remove existing entry for this username if it exists
                if 'Username' in existing_df.columns and new_username in existing_df['Username'].values:
                    existing_df = existing_df[existing_df['Username'] != new_username]
                
                # Append the new data
                combined_df = pd.concat([existing_df, df], ignore_index=True)
                
                # Save the combined data
                combined_df.to_csv(csv_path, index=False)
                #print(f"Data updated in {csv_path}")
                
            except Exception as e:
                print(f"Error reading existing file: {e}")
                # Backup the existing file and create new
                backup_path = f"{csv_path}.bak"
                try:
                    os.rename(csv_path, backup_path)
                    print(f"Created backup of existing file at {backup_path}")
                except:
                    pass
                
                # Create new file
                df.to_csv(csv_path, index=False)
                print(f"Created new CSV file at {csv_path}")
        else:
            # Create new file
            df.to_csv(csv_path, index=False)
            print(f"Created new CSV file at {csv_path}")
            
    except Exception as e:
        print(f"Error saving to CSV: {e}")
#-------------------------------------------------------------------------------------------------------------

# Constants
UNOFFICIAL_LEETCODE_API_ENDPOINT = "https://leetcode.com/graphql"  # Unofficial API endpoint
LEETCODE_SESSION_TOKEN = "zzfDvscjPpc5ayBR8y5LULqszG6qfKhn40z7hHwNmMi7VuDRiLENZxBxCK7HAtIi"  # Replace with your actual session token

# Headers setup
HEADERS = {
    "Content-Type": "application/json",
    "Referer": "https://leetcode.com",
    "User-Agent": "Mozilla/5.0",
    "Cookie": f"LEETCODE_SESSION={LEETCODE_SESSION_TOKEN}; csrftoken={LEETCODE_SESSION_TOKEN}"
}

def fetch_user_profile(username):
    """Fetch basic user profile information using the unofficial API."""
    query = """
    query getUserProfile($username: String!) {
      matchedUser(username: $username) {
        username
        profile {
          realName
          userAvatar
          ranking
          reputation
          postViewCount
          solutionCount
        }
        submitStats {
          acSubmissionNum {
            difficulty
            count
            submissions
          }
        }
      }
    }
    """

    variables = {
        "username": username
    }

    #print(f"Fetching profile for {username}...")
    response = requests.post(
        UNOFFICIAL_LEETCODE_API_ENDPOINT,
        headers=HEADERS,
        json={"query": query, "variables": variables}
    )

    if response.status_code == 200:
        data = response.json()
        #print(f"Profile data for {username}: {data}")
        return data.get("data", {}).get("matchedUser", {})
    else:
        #print(f"Error fetching profile for {username}: {response.status_code}")
        #print(f"Response: {response.text}")  # Print the response for debugging
        return {}

def fetch_contest_history(username):
    """Fetch user's contest participation history using the unofficial API."""
    query = """
    query getUserContestRanking($username: String!) {
      userContestRanking(username: $username) {
        attendedContestsCount
        rating
        globalRanking
        totalParticipants
        topPercentage
      }
      userContestRankingHistory(username: $username) {
        attended
        problemsSolved
        totalProblems
        rating
        ranking
        contest {
          title
          startTime
        }
      }
    }
    """

    variables = {
        "username": username
    }

    #print(f"Fetching contest history for {username}...")
    response = requests.post(
        UNOFFICIAL_LEETCODE_API_ENDPOINT,
        headers=HEADERS,
        json={"query": query, "variables": variables}
    )

    if response.status_code == 200:
        data = response.json()
        #print(f"Contest history data for {username}: {data}")
        return data.get("data", {})
    else:
        #print(f"Error fetching contest history for {username}: {response.status_code}")
        #print(f"Response: {response.text}")  # Print the response for debugging
        return {}

def fetch_solved_problems(username):
    """Fetch the list of problems solved by the user using the unofficial API."""
    query = """
    query getUserSolvedProblems($username: String!) {
      matchedUser(username: $username) {
        submitStats {
          acSubmissionNum {
            difficulty
            count
            submissions
          }
        }
      }
    }
    """

    variables = {
        "username": username
    }

    #print(f"Fetching solved problems for {username}...")
    response = requests.post(
        UNOFFICIAL_LEETCODE_API_ENDPOINT,
        headers=HEADERS,
        json={"query": query, "variables": variables}
    )

    if response.status_code == 200:
        data = response.json()
        #print(f"Solved problems data for {username}: {data}")
        return data.get("data", {}).get("matchedUser", {})
    else:
        #print(f"Error fetching solved problems for {username}: {response.status_code}")
        #print(f"Response: {response.text}")  # Print the response for debugging
        return {}

def fetch_activity_calendar(username):
    """Fetch user's activity calendar (heatmap data) using the unofficial API."""
    query = """
    query getUserActivityCalendar($username: String!, $year: Int!) {
      matchedUser(username: $username) {
        userCalendar(year: $year) {
          submissionCalendar
        }
      }
    }
    """

    # Get current year
    current_year = datetime.now().year

    variables = {
        "username": username,
        "year": current_year
    }

    #print(f"Fetching activity calendar for {username}...")
    response = requests.post(
        UNOFFICIAL_LEETCODE_API_ENDPOINT,
        headers=HEADERS,
        json={"query": query, "variables": variables}
    )

    if response.status_code == 200:
        data = response.json()
        #print(f"Activity calendar data for {username}: {data}")

        # Extract the userCalendar object from the nested structure
        user_calendar = data.get("data", {}).get("matchedUser", {}).get("userCalendar", {})
        return user_calendar
    else:
        #print(f"Error fetching activity calendar for {username}: {response.status_code}")
        #print(f"Response: {response.text}")  # Print the response for debugging
        return {}

def calculate_problem_solving_score(solved_problems):
    """Calculate problem solving score based on difficulty and count with progressive scaling."""
    if not solved_problems:
        return 0

    submission_stats = solved_problems.get("submitStats", {}).get("acSubmissionNum", [])

    easy_count = medium_count = hard_count = 0

    for stat in submission_stats:
        difficulty = stat.get("difficulty")
        count = stat.get("count", 0)

        if difficulty == "Easy":
            easy_count = count
        elif difficulty == "Medium":
            medium_count = count
        elif difficulty == "Hard":
            hard_count = count

    # Enhanced weighting with diminishing returns for volume
    # First few problems of each type count more, then diminishing returns kick in
    easy_score = min(30, easy_count) + max(0, (easy_count - 30) * 0.5)
    medium_score = min(20, medium_count) * 3 + max(0, (medium_count - 20) * 1.5)
    hard_score = min(10, hard_count) * 5 + max(0, (hard_count - 10) * 2.5)

    # Calculate problem solving score with progressive scaling
    problem_solving_score = easy_score + medium_score + hard_score

    # Bonus for balance across difficulty levels
    if easy_count > 0 and medium_count > 0 and hard_count > 0:
        balance_bonus = min(easy_count, medium_count, hard_count) * 2
        problem_solving_score += balance_bonus

    return problem_solving_score



def calculate_contest_score(contest_history):
    """Calculate contest participation score with emphasis on rating and consistency."""
    if not contest_history:
        return 0

    ranking_info = contest_history.get("userContestRanking", {})
    # Add safety check for None
    if ranking_info is None:
        return 0

    contest_count = ranking_info.get("attendedContestsCount", 0)
    rating = ranking_info.get("rating", 0)
    global_ranking = ranking_info.get("globalRanking", 0)
    total_participants = ranking_info.get("totalParticipants", 1)
    top_percentage = ranking_info.get("topPercentage", 100)

    # Get contest history details for consistency analysis
    contest_history_list = contest_history.get("userContestRankingHistory", [])

    # Calculate base contest score
    # Base participation score
    participation_score = min(20, contest_count) * 5 + max(0, (contest_count - 20)) * 2

    # Rating score (non-linear to reward higher ratings)
    rating_score = 0
    if rating > 0:
        if rating < 1500:
            rating_score = rating * 0.05
        elif rating < 2000:
            rating_score = 75 + (rating - 1500) * 0.1
        elif rating < 2500:
            rating_score = 125 + (rating - 2000) * 0.2
        else:
            rating_score = 225 + (rating - 2500) * 0.3

    # Percentile score (better rank = higher score)
    percentile_score = (100 - top_percentage) * 1.5

    # Consistency bonus (reward regular participation)
    consistency_bonus = 0
    if len(contest_history_list) >= 3:
        # Bonus for participating in at least 3 contests
        consistency_bonus = 20

    if len(contest_history_list) >= 10:
        # Additional bonus for participating in many contests
        consistency_bonus += 30

    # Problem solving rate in contests
    problem_solving_rate = 0
    problems_solved = 0
    total_problems = 0

    for contest in contest_history_list:
        problems_solved += contest.get("problemsSolved", 0)
        total_problems += contest.get("totalProblems", 0)

    if total_problems > 0:
        problem_solving_rate = (problems_solved / total_problems) * 100
        # Bonus for high problem-solving rate
        problem_solving_bonus = problem_solving_rate * 2
    else:
        problem_solving_bonus = 0

    # Calculate final contest score
    contest_score = (
        participation_score +
        rating_score +
        percentile_score +
        consistency_bonus +
        problem_solving_bonus
    )

    return contest_score

def calculate_consistency_score(activity_calendar):
    """Calculate consistency score based on activity calendar with emphasis on streaks and recent activity."""
    if not activity_calendar:
        #print("Activity calendar is empty or None. Returning consistency score 0.")
        return 0

    # Safely get the submissionCalendar string
    submission_calendar_str = activity_calendar.get("submissionCalendar", "{}")

    # Parse the submissionCalendar string into a dictionary
    try:
        submission_calendar = json.loads(submission_calendar_str)
    except json.JSONDecodeError:
        #print("Failed to parse submissionCalendar. Using empty dictionary.")
        submission_calendar = {}

    # If submission calendar is empty, return 0
    if not submission_calendar:
        #print("Submission calendar is empty. Returning consistency score 0.")
        return 0

    # Calculate activity metrics
    now = datetime.now()

    # Track activity in different time periods
    active_days_last_week = 0
    active_days_last_month = 0
    active_days_last_quarter = 0

    # Track streaks
    current_streak = 0
    max_streak = 0
    streak_days = []

    for timestamp_str, count in submission_calendar.items():
        try:
            # Convert timestamp to datetime
            timestamp = int(timestamp_str)
            day_time = datetime.fromtimestamp(timestamp)
            days_ago = (now - day_time).days

            # Count active days in different periods
            if int(count) > 0:
                if 0 <= days_ago <= 7:
                    active_days_last_week += 1
                if 0 <= days_ago <= 30:
                    active_days_last_month += 1
                if 0 <= days_ago <= 90:
                    active_days_last_quarter += 1

                # Track days for streak calculation
                streak_days.append(day_time.date())

        except (ValueError, TypeError) as e:
            #print(f"Error processing timestamp {timestamp_str}: {e}")
            continue

    # Sort streak days
    streak_days.sort()

    # Calculate streaks
    if streak_days:
        temp_streak = 1
        for i in range(1, len(streak_days)):
            # Check if consecutive days
            if (streak_days[i] - streak_days[i-1]).days == 1:
                temp_streak += 1
            else:
                # Reset streak if gap
                max_streak = max(max_streak, temp_streak)
                temp_streak = 1

        # Update max streak
        max_streak = max(max_streak, temp_streak)

        # Calculate current streak
        # Check if most recent day is today or yesterday
        most_recent = streak_days[-1]
        today = datetime.now().date()
        yesterday = today - timedelta(days=1)

        if most_recent == today or most_recent == yesterday:
            current_streak = temp_streak
        else:
            current_streak = 0

    # Calculate consistency score
    recent_activity_score = (
        active_days_last_week * 10 +  # Heavy weight on recent activity
        active_days_last_month * 2 +  # Medium weight on monthly activity
        active_days_last_quarter * 0.5  # Light weight on quarterly activity
    )

    streak_score = (
        current_streak * 5 +  # Reward current streak
        max_streak * 2  # Reward best streak
    )

    # Cap consistency score
    consistency_score = min(300, recent_activity_score + streak_score)

    return consistency_score

def calculate_leetcode_score(username):
    """Calculate LeetCode profile score for a single username with improved scoring algorithm."""
    results = []

    # Define maximum theoretical scores for normalization
    MAX_PROBLEM_SOLVING_SCORE = 5000  # Adjusted based on your scoring algorithm
    MAX_CONTEST_SCORE = 700          # Adjusted based on your scoring algorithm
    MAX_CONSISTENCY_SCORE = 200       # From your capped consistency score

    try:
        # Fetch all data
        profile = fetch_user_profile(username)
        contest_history = fetch_contest_history(username)
        solved_problems = fetch_solved_problems(username)
        activity_calendar = fetch_activity_calendar(username)

        # Check if profile data is available
        if not profile:
            print(f"Could not fetch profile for {username}")
            return results

        # Basic information
        user_data = {
            "username": username,
            "real_name": profile.get("profile", {}).get("realName", ""),
            "ranking": profile.get("profile", {}).get("ranking", 0),
            "reputation": profile.get("profile", {}).get("reputation", 0)
        }

        # Problem solving stats
        problem_stats = profile.get("submitStats", {}).get("acSubmissionNum", [])
        for stat in problem_stats:
            difficulty = stat.get("difficulty", "")
            count = stat.get("count", 0)
            if difficulty:
                user_data[f"{difficulty.lower()}_solved"] = count

        # Ensure all difficulty levels are represented
        for difficulty in ["easy", "medium", "hard"]:
            if f"{difficulty}_solved" not in user_data:
                user_data[f"{difficulty}_solved"] = 0

        user_data["total_solved"] = user_data.get("easy_solved", 0) + user_data.get("medium_solved", 0) + user_data.get("hard_solved", 0)

        # Contest stats
        contest_ranking = contest_history.get("userContestRanking", {})
        if contest_ranking is None:
            contest_ranking = {}

        user_data["contest_rating"] = contest_ranking.get("rating", 0)
        user_data["contests_attended"] = contest_ranking.get("attendedContestsCount", 0)
        user_data["contest_global_rank"] = contest_ranking.get("globalRanking", 0)

        # Activity stats
        user_data["active_days"] = 0
        user_data["current_streak"] = 0
        user_data["max_streak"] = 0

        if activity_calendar:
            submission_calendar_str = activity_calendar.get("submissionCalendar", "{}")
            try:
                submission_calendar = json.loads(submission_calendar_str)
                user_data["active_days"] = len(submission_calendar)

                # Calculate streaks
                streak_data = calculate_streaks(submission_calendar)
                user_data["current_streak"] = streak_data.get("current_streak", 0)
                user_data["max_streak"] = streak_data.get("max_streak", 0)

            except json.JSONDecodeError:
                print(f"Failed to parse submissionCalendar for {username}. Using empty dictionary.")

        # Calculate component scores with improved algorithms
        problem_solving_score = calculate_problem_solving_score(solved_problems)
        contest_score = calculate_contest_score(contest_history)
        consistency_score = calculate_consistency_score(activity_calendar)

        # Calculate normalized component scores (0-100 scale)
        normalized_problem_score = min(100, (problem_solving_score / MAX_PROBLEM_SOLVING_SCORE) * 100)
        normalized_contest_score = min(100, (contest_score / MAX_CONTEST_SCORE) * 100)
        normalized_consistency_score = min(100, (consistency_score / MAX_CONSISTENCY_SCORE) * 100)

        # Calculate new metrics
        difficulty_ratio = 0
        if user_data["total_solved"] > 0:
            weighted_sum = (user_data["easy_solved"] * 1 +
                           user_data["medium_solved"] * 2 +
                           user_data["hard_solved"] * 3)
            difficulty_ratio = weighted_sum / (user_data["total_solved"] * 3)
            user_data["difficulty_ratio"] = round(difficulty_ratio, 2)

        # Calculate total normalized score with weighting
        normalized_total_score = (
            (normalized_problem_score * 0.45) +  # Problems are most important
            (normalized_contest_score * 0.35) +  # Contests show competitive skills
            (normalized_consistency_score * 0.2)  # Consistency shows dedication
        )

        # Give bonus for well-rounded profiles (capped at 5 points)
        if normalized_problem_score > 0 and normalized_contest_score > 0 and normalized_consistency_score > 0:
            min_score = min(normalized_problem_score, normalized_contest_score, normalized_consistency_score)
            balance_bonus = min(5, min_score * 0.05)
            normalized_total_score = min(100, normalized_total_score + balance_bonus)

        # Store both raw and normalized scores
        user_data["problem_solving_score_raw"] = round(problem_solving_score, 2)
        user_data["contest_score_raw"] = round(contest_score, 2)
        user_data["consistency_score_raw"] = round(consistency_score, 2)

        user_data["problem_solving_score"] = round(normalized_problem_score, 1)
        user_data["contest_score"] = round(normalized_contest_score, 1)
        user_data["consistency_score"] = round(normalized_consistency_score, 1)
        user_data["leetcode_score"] = round(normalized_total_score, 3)

        # Calculate level/tier based on normalized total score
        user_data["skill_level"] = calculate_skill_level(normalized_total_score)

        results.append(user_data)

    except Exception as e:
        print(f"Error analyzing {username}: {str(e)}")
        import traceback
        traceback.print_exc()  # More detailed error information

    return results


def calculate_streaks(submission_calendar):
    """Calculate current and maximum streaks from submission calendar."""
    if not submission_calendar:
        return {"current_streak": 0, "max_streak": 0}

    # Convert timestamps to dates
    active_dates = []
    try:
        for timestamp_str, count in submission_calendar.items():
            if int(count) > 0:
                day_time = datetime.fromtimestamp(int(timestamp_str))
                active_dates.append(day_time.date())
    except (ValueError, TypeError) as e:
        #print(f"Error processing timestamps: {e}")
        return {"current_streak": 0, "max_streak": 0}

    # Sort dates
    active_dates.sort()

    if not active_dates:
        return {"current_streak": 0, "max_streak": 0}

    # Calculate max streak
    max_streak = 1
    current_streak = 1

    for i in range(1, len(active_dates)):
        if (active_dates[i] - active_dates[i-1]).days == 1:
            current_streak += 1
        else:
            max_streak = max(max_streak, current_streak)
            current_streak = 1

    max_streak = max(max_streak, current_streak)

    # Check if current streak is active (today or yesterday)
    today = datetime.now().date()
    yesterday = today - timedelta(days=1)

    if active_dates[-1] == today or active_dates[-1] == yesterday:
        current_active_streak = current_streak
    else:
        current_active_streak = 0

    return {
        "current_streak": current_active_streak,
        "max_streak": max_streak
    }

def calculate_skill_level(normalized_score):
    """Determine skill level based on normalized total score (0-100)."""
    if normalized_score < 20:
        return "Beginner"
    elif normalized_score < 40:
        return "Intermediate"
    elif normalized_score < 60:
        return "Advanced"
    elif normalized_score < 80:
        return "Expert"
    elif normalized_score < 95:
        return "Master"
    else:
        return "Grandmaster"

def run_leetcode_analysis(username):
    """Run the LeetCode analysis for a single username and return a DataFrame."""
    results = calculate_leetcode_score(username)

    if not results:
        return None  # No data collected

    df = pd.DataFrame(results)

    # Reorganize columns for better readability
    column_order = [
        "username", "real_name", "ranking", "reputation",
        "total_solved", "easy_solved", "medium_solved", "hard_solved",
        "contests_attended", "contest_rating", "contest_global_rank",
        "active_days", "current_streak", "max_streak",
        "problem_solving_score", "contest_score", "consistency_score",
        "leetcode_score", "skill_level"
    ]

    # Only include columns that exist in the DataFrame
    ordered_columns = [col for col in column_order if col in df.columns]
    df = df[ordered_columns]

    # Ensure CSV file exists before reading
    if os.path.isfile("leetcode_analysis.csv"):
        try:
            # Read existing data
            existing_df = pd.read_csv("leetcode_analysis.csv")

            # Ensure consistent column names
            existing_df.columns = existing_df.columns.str.strip()
            df.columns = df.columns.str.strip()

            # Append the new data
            combined_df = pd.concat([existing_df, df], ignore_index=True)

            # Remove duplicates based on 'username' to avoid adding the same user multiple times
            combined_df = combined_df.drop_duplicates(subset=["username"], keep="last")

            # Save the combined data back to the CSV file
            combined_df.to_csv("leetcode_analysis.csv", index=False)
        except Exception as e:
            print(f"Error reading existing file: {e}")
            df.to_csv("leetcode_analysis.csv", index=False)  # If an error occurs, save new data
    else:
        # Save the new data to the CSV file
        df.to_csv("leetcode_analysis.csv", index=False)

    return df

#-----------------------------------------------------------------------------------------------------------------

# Configure logging to suppress output
logging.basicConfig(level=logging.CRITICAL)

# Clone GitHub Repository
def clone_github_repo(repo_url, repo_name=None):
    """Clone a GitHub repository to a local directory."""
    if not repo_name:
        repo_name = repo_url.split("/")[-1].replace(".git", "")

    repo_path = os.path.join(os.getcwd(), repo_name)
    if os.path.exists(repo_path):
        shutil.rmtree(repo_path)  # Clean up if repo exists

    try:
        subprocess.run(
            ["git", "clone", repo_url, repo_name],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            check=True
        )
        return repo_path
    except subprocess.CalledProcessError:
        return None

# Extract Code Files
def extract_code_files(repo_path, extensions=None):
    """Find all code files with specified extensions in the repository."""
    if extensions is None:
        extensions = [
            ".py", ".js", ".ts", ".tsx", ".jsx", ".java", ".go",
            ".c", ".cpp", ".cs", ".php", ".rb", ".swift", ".kt",
            ".rs", ".scala", ".sh", ".bash", ".pl", ".r", ".lua",
            ".html", ".css", ".scss", ".less", ".vue", ".svelte",
            ".json", ".xml", ".yaml", ".yml"
        ]

    code_files = []
    ignored_dirs = [".git", "node_modules", "venv", ".env", "dist", "build", "__pycache__"]

    try:
        for root, dirs, files in os.walk(repo_path):
            dirs[:] = [d for d in dirs if d not in ignored_dirs]
            for file in files:
                if any(file.endswith(ext) for ext in extensions):
                    code_files.append(os.path.join(root, file))
        return code_files
    except Exception:
        return []

# Run a shell command
def run_command(cmd, file_path, timeout=60):
    """Run a shell command with proper error handling and timeout."""
    try:
        result = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=timeout,
            check=False
        )
        return result.returncode, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return 1, "", f"Command timed out after {timeout} seconds"
    except Exception:
        return 1, "", "Command failed"

# Analyze Python Code
def analyze_python_file(file_path):
    """Analyze a Python file using various tools."""
    results = defaultdict(int)
    file_size = os.path.getsize(file_path)

    if file_size > 1000000:  # 1MB
        results["skipped"] = 1
        return results

    # Run flake8
    returncode, stdout, stderr = run_command(["flake8", file_path], file_path)
    results["flake8_issues"] = len(stdout.splitlines())

    # Run bandit
    returncode, stdout, stderr = run_command(["bandit", "-q", "-f", "json", file_path], file_path)
    try:
        if stdout.strip():
            bandit_issues = json.loads(stdout)
            results["bandit_issues"] = len(bandit_issues.get("results", []))
    except json.JSONDecodeError:
        results["bandit_issues"] = 0

    # Run radon
    returncode, stdout, stderr = run_command(["radon", "cc", "-a", file_path], file_path)
    results["radon_complexity"] = len(stdout.splitlines())

    # Calculate average complexity
    total_complexity = 0
    complexity_count = 0
    for line in stdout.splitlines():
        parts = line.split(" - ")
        if len(parts) >= 2:
            try:
                complexity = parts[1].strip()[0]
                if complexity.isalpha():
                    complexity_value = ord('Z') - ord(complexity.upper())
                    total_complexity += complexity_value
                    complexity_count += 1
            except IndexError:
                pass

    if complexity_count > 0:
        results["avg_complexity"] = total_complexity / complexity_count
    else:
        results["avg_complexity"] = 0

    # Run pydocstyle
    returncode, stdout, stderr = run_command(["pydocstyle", file_path], file_path)
    results["pydocstyle_issues"] = len(stdout.splitlines())

    # Run mypy
    returncode, stdout, stderr = run_command(["mypy", file_path], file_path)
    results["mypy_issues"] = len(stdout.splitlines())

    return results

# Analyze JavaScript/TypeScript Code
def analyze_javascript_file(file_path):
    """Analyze JavaScript or TypeScript files."""
    results = defaultdict(int)
    file_size = os.path.getsize(file_path)

    if file_size > 1000000:  # 1MB
        results["skipped"] = 1
        return results

    # Create a temporary ESLint config
    eslint_config = {
        "env": {"browser": True, "es2021": True, "node": True},
        "extends": "eslint:recommended",
        "parserOptions": {"ecmaVersion": "latest", "sourceType": "module"},
        "rules": {}
    }

    with tempfile.NamedTemporaryFile('w', suffix='.json', delete=False) as tmp:
        tmp_config_path = tmp.name
        json.dump(eslint_config, tmp)

    try:
        # Run ESLint
        returncode, stdout, stderr = run_command(
            ["eslint", "-c", tmp_config_path, "--format", "json", file_path],
            file_path
        )
        try:
            if stdout.strip():
                eslint_issues = json.loads(stdout)
                if isinstance(eslint_issues, list) and len(eslint_issues) > 0:
                    results["eslint_issues"] = sum(len(file_issues.get("messages", [])) for file_issues in eslint_issues)
        except json.JSONDecodeError:
            results["eslint_issues"] = 0
    finally:
        # Clean up temp file
        try:
            os.unlink(tmp_config_path)
        except:
            pass

    return results

# Analyze Java Code
def analyze_java_file(file_path):
    """Analyze Java files."""
    results = defaultdict(int)
    file_size = os.path.getsize(file_path)

    if file_size > 1000000:  # 1MB
        results["skipped"] = 1
        return results

    # Check if Checkstyle JAR exists
    checkstyle_jar = "checkstyle-10.12.0-all.jar"
    if not os.path.exists(checkstyle_jar):
        return results

    # Create a temporary Checkstyle config
    google_checks_xml = """<?xml version="1.0"?>
<!DOCTYPE module PUBLIC "-//Checkstyle//DTD Checkstyle Configuration 1.3//EN" "https://checkstyle.org/dtds/configuration_1_3.dtd">
<module name="Checker">
  <module name="TreeWalker">
    <module name="UnusedImports"/>
    <module name="RedundantImport"/>
    <module name="AvoidStarImport"/>
  </module>
</module>
"""

    with tempfile.NamedTemporaryFile('w', suffix='.xml', delete=False) as tmp:
        tmp_config_path = tmp.name
        tmp.write(google_checks_xml)

    try:
        # Run Checkstyle
        returncode, stdout, stderr = run_command(
            ["java", "-jar", checkstyle_jar, "-c", tmp_config_path, file_path],
            file_path
        )
        results["checkstyle_issues"] = len(stdout.splitlines())
    finally:
        # Clean up temp file
        try:
            os.unlink(tmp_config_path)
        except:
            pass

    return results

# Analyze Go Code
def analyze_go_file(file_path):
    """Analyze Go files."""
    results = defaultdict(int)
    file_size = os.path.getsize(file_path)

    if file_size > 1000000:  # 1MB
        results["skipped"] = 1
        return results

    # Run golint
    returncode, stdout, stderr = run_command(["golint", file_path], file_path)
    if returncode != 127:  # Command exists
        results["golint_issues"] = len(stdout.splitlines())

    # Run gosec
    returncode, stdout, stderr = run_command(["gosec", "-fmt=json", file_path], file_path)
    if returncode != 127:  # Command exists
        try:
            if stdout.strip():
                gosec_issues = json.loads(stdout)
                results["gosec_issues"] = len(gosec_issues.get("Issues", []))
        except json.JSONDecodeError:
            results["gosec_issues"] = 0

    # Run staticcheck
    returncode, stdout, stderr = run_command(["staticcheck", file_path], file_path)
    if returncode != 127:  # Command exists
        results["staticcheck_issues"] = len(stdout.splitlines())

    # Run gofmt
    returncode, stdout, stderr = run_command(["gofmt", "-l", file_path], file_path)
    results["gofmt_issues"] = 1 if stdout.strip() else 0

    return results

# Analyze C/C++ Code
def analyze_cpp_file(file_path):
    """Analyze C/C++ files."""
    results = defaultdict(int)
    file_size = os.path.getsize(file_path)

    if file_size > 1000000:  # 1MB
        results["skipped"] = 1
        return results

    # Run cppcheck
    returncode, stdout, stderr = run_command(["cppcheck", "--enable=all", "--xml", file_path], file_path)
    if returncode != 127:  # Command exists
        results["cppcheck_issues"] = stderr.count("<error ")

    # Run clang-tidy
    returncode, stdout, stderr = run_command(["clang-tidy", file_path, "--"], file_path)
    if returncode != 127:  # Command exists
        results["clang_tidy_issues"] = stdout.count(": warning:") + stdout.count(": error:")

    return results

# Analyze PHP Code
def analyze_php_file(file_path):
    """Analyze PHP files."""
    results = defaultdict(int)
    file_size = os.path.getsize(file_path)

    if file_size > 1000000:  # 1MB
        results["skipped"] = 1
        return results

    # Run PHP syntax check
    returncode, stdout, stderr = run_command(["php", "-l", file_path], file_path)
    results["php_syntax_issues"] = 1 if "Errors parsing" in stderr else 0

    # Run PHP Code Sniffer
    returncode, stdout, stderr = run_command(["phpcs", "--report=json", file_path], file_path)
    if returncode != 127:  # Command exists
        try:
            if stdout.strip():
                phpcs_issues = json.loads(stdout)
                files = phpcs_issues.get("files", {})
                total_issues = 0
                for file_issues in files.values():
                    total_issues += len(file_issues.get("messages", []))
                results["phpcs_issues"] = total_issues
        except json.JSONDecodeError:
            results["phpcs_issues"] = 0

    return results

# Analyze Ruby Code
def analyze_ruby_file(file_path):
    """Analyze Ruby files."""
    results = defaultdict(int)
    file_size = os.path.getsize(file_path)

    if file_size > 1000000:  # 1MB
        results["skipped"] = 1
        return results

    # Run Ruby syntax check
    returncode, stdout, stderr = run_command(["ruby", "-c", file_path], file_path)
    results["ruby_syntax_issues"] = 0 if "Syntax OK" in stdout else 1

    # Run Rubocop
    returncode, stdout, stderr = run_command(["rubocop", "--format", "json", file_path], file_path)
    if returncode != 127:  # Command exists
        try:
            if stdout.strip():
                rubocop_issues = json.loads(stdout)
                total_issues = 0
                for file_issues in rubocop_issues.get("files", []):
                    total_issues += len(file_issues.get("offenses", []))
                results["rubocop_issues"] = total_issues
        except json.JSONDecodeError:
            results["rubocop_issues"] = 0

    return results

# Analyze Swift Code
def analyze_swift_file(file_path):
    """Analyze Swift files."""
    results = defaultdict(int)
    file_size = os.path.getsize(file_path)

    if file_size > 1000000:  # 1MB
        results["skipped"] = 1
        return results

    # Run SwiftLint
    returncode, stdout, stderr = run_command(["swiftlint", "lint", "--reporter", "json", file_path], file_path)
    if returncode != 127:  # Command exists
        try:
            if stdout.strip():
                swiftlint_issues = json.loads(stdout)
                results["swiftlint_issues"] = len(swiftlint_issues)
        except json.JSONDecodeError:
            results["swiftlint_issues"] = 0

    return results

# Analyze any text file for general metrics
def analyze_general_text_file(file_path):
    """Analyze any text file for general metrics."""
    results = defaultdict(int)
    file_size = os.path.getsize(file_path)

    if file_size > 1000000:  # 1MB
        results["skipped"] = 1
        return results

    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
            lines = content.splitlines()
            results["total_lines"] = len(lines)
            results["non_empty_lines"] = sum(1 for line in lines if line.strip())
            comment_patterns = ['#', '//', '/*', '*/', '"""', "'''", '--']
            comment_lines = sum(1 for line in lines if any(pattern in line for pattern in comment_patterns))
            results["estimated_comment_lines"] = comment_lines
            if results["non_empty_lines"] > 0:
                results["comment_ratio"] = comment_lines / results["non_empty_lines"]
            results["long_lines"] = sum(1 for line in lines if len(line) > 100)
            results["todos"] = sum(1 for line in lines if 'TODO' in line)
            results["fixmes"] = sum(1 for line in lines if 'FIXME' in line)
    except Exception:
        pass

    return results

# Select appropriate analyzer based on file extension
def analyze_file(file_path):
    """Choose appropriate analyzer based on file extension."""
    file_ext = os.path.splitext(file_path)[1].lower()
    results = analyze_general_text_file(file_path)

    if file_ext == '.py':
        python_results = analyze_python_file(file_path)
        results.update(python_results)
    elif file_ext in ['.js', '.ts', '.jsx', '.tsx']:
        js_results = analyze_javascript_file(file_path)
        results.update(js_results)
    elif file_ext == '.java':
        java_results = analyze_java_file(file_path)
        results.update(java_results)
    elif file_ext == '.go':
        go_results = analyze_go_file(file_path)
        results.update(go_results)
    elif file_ext in ['.c', '.cpp', '.cc', '.h', '.hpp']:
        cpp_results = analyze_cpp_file(file_path)
        results.update(cpp_results)
    elif file_ext == '.php':
        php_results = analyze_php_file(file_path)
        results.update(php_results)
    elif file_ext == '.rb':
        ruby_results = analyze_ruby_file(file_path)
        results.update(ruby_results)
    elif file_ext == '.swift':
        swift_results = analyze_swift_file(file_path)
        results.update(swift_results)

    return results

def remove_readonly(func, path, _):
    """Clears the read-only attribute and retries deletion."""
    os.chmod(path, 0o777)  # Grant full permissions
    func(path)

def evaluate_repository(repo_url, max_workers=None):
    """Evaluate an entire repository and return the final score."""
    repo_path = clone_github_repo(repo_url)
    if not repo_path:
        return 0

    code_files = extract_code_files(repo_path)
    if not code_files:
        shutil.rmtree(repo_path, onerror=remove_readonly)  # Fix deletion issue
        return 0

    results_by_language = defaultdict(lambda: defaultdict(float))
    all_issue_types = set()

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_file = {executor.submit(analyze_file, file): file for file in code_files}
        for future in concurrent.futures.as_completed(future_to_file):
            file = future_to_file[future]
            ext = os.path.splitext(file)[1].lower()
            try:
                file_results = future.result()
                for key, value in file_results.items():
                    results_by_language[ext][key] += value
                    all_issue_types.add(key)
            except Exception:
                pass

    for ext, metrics in results_by_language.items():
        file_count = len([f for f in code_files if os.path.splitext(f)[1].lower() == ext])
        if file_count > 0:
            for key in metrics:
                metrics[key] /= file_count

    total_results = defaultdict(float)
    for ext, metrics in results_by_language.items():
        language_weight = len([f for f in code_files if os.path.splitext(f)[1].lower() == ext]) / len(code_files)
        for key, value in metrics.items():
            total_results[key] += value * language_weight

    final_score = calculate_final_score(total_results)

    #  Ensure `.git` directory is writable before deletion
    git_path = os.path.join(repo_path, ".git")
    if os.path.exists(git_path):
        shutil.rmtree(git_path, onerror=remove_readonly)

    # Clean up cloned repository
    shutil.rmtree(repo_path, onerror=remove_readonly)  #  Fix permission error

    return final_score

# Calculate final quality score
def calculate_final_score(total_results):
    """Calculate a final quality score based on the analysis results."""
    score_weights = {
        # Python-specific
        "flake8_issues": -0.5,
        "bandit_issues": -1.0,
        "radon_complexity": -0.5,
        "pydocstyle_issues": -0.3,
        "mypy_issues": -0.5,

        # JavaScript/TypeScript
        "eslint_issues": -0.5,

        # Java
        "checkstyle_issues": -0.5,

        # Go
        "golint_issues": -0.5,
        "gosec_issues": -1.0,
        "staticcheck_issues": -0.5,
        "gofmt_issues": -0.3,

        # C/C++
        "cppcheck_issues": -0.7,
        "clang_tidy_issues": -0.5,

        # PHP
        "php_syntax_issues": -1.0,
        "phpcs_issues": -0.5,

        # Ruby
        "ruby_syntax_issues": -1.0,
        "rubocop_issues": -0.5,

        # Swift
        "swiftlint_issues": -0.5,

        # General metrics
        "comment_ratio": 2.0,  # Higher is better
        "long_lines": -0.2,
        "todos": -0.1,
        "fixmes": -0.2,
    }

    base_score = 80
    score_impact = 0
    for metric, value in total_results.items():
        if metric in score_weights:
            weight = score_weights[metric]
            adjusted_value = min(value, 20)  # Cap at 20 issues per file on average
            score_impact += adjusted_value * weight

    final_score = max(0, min(100, base_score + score_impact))
    return final_score

#------------------------------------------------------------------------------------------------------------------
def evaluate_income(income_rupees):
    """
    Evaluate developer's income and assign a score.

    Parameters:
    income_rupees (float or int): Annual income in rupees (e.g., 85000 for 85,000)

    Returns:
    float: Income score out of 100
    """
    # Convert income to lakhs (1 lakh = 100,000 rupees)
    income_lakhs = income_rupees / 100000.0

    # Maximum threshold is 5 lakhs (500,000 rupees)
    max_lakhs = 5.0
    min_lakhs = 1.0  # 1 lakh (100,000 rupees)

    # Automatic 100 score for income below 1 lakh
    if income_lakhs < min_lakhs:
        #print(f"Income Evaluation: {income_rupees:,.2f} ({income_lakhs:.2f}L) --> Score: 100.00/100 (Below 1L threshold)")
        return 100.0

    # Check if income exceeds maximum threshold
    if income_lakhs > max_lakhs:
        #print(f"Income ({income_rupees:,.2f} or {income_lakhs:.2f}L) exceeds the maximum threshold of 5L")
        return 0.0

    # Score based on income - inverse linear scaling between 1L and 5L
    # 1L = 100 points, 5L = 0 points
    score = 100 - ((income_lakhs - min_lakhs) / (max_lakhs - min_lakhs) * 100)

    #print(f"Income Evaluation: {income_rupees:,.2f} ({income_lakhs:.2f}L) --> Score: {score:.2f}/100")
    return score
#--------------------------------------------------------------------------------------------------------------------------


def validate_repo_ownership(repo_url, github_username):
    """
    Validates if a repository belongs to the specified GitHub username.

    Parameters:
    repo_url (str): URL of the repository to check
    github_username (str): GitHub username to verify against

    Returns:
    bool: True if the repository belongs to the user, False otherwise
    """
    # Basic URL structure validation
    if not repo_url or not github_username:
        return False

    # Extract username from repository URL
    # Format: https://github.com/username/repository.git
    try:
        # Remove http:// or https:// and potential www.
        clean_url = repo_url.replace("https://", "").replace("http://", "").replace("www.", "")

        # Split by '/' to get the path components
        parts = clean_url.split('/')

        # GitHub URLs have the username as the first component after the domain
        if len(parts) >= 2 and parts[0] == "github.com":
            repo_owner = parts[1]
            return repo_owner.lower() == github_username.lower()

    except Exception:
        return False

    return False
#---------------------------------------------------------------------------------------------------------------------

def determine_ai_weights(github_score, leetcode_score, repo1_score, repo2_score, income_score):
    """
    Determines optimal weights for each factor based on developer profile.
    Uses a simple AI approach to adjust weights based on relative strengths.

    Parameters:
    github_score (float): GitHub profile score
    leetcode_score (float): LeetCode profile score
    repo1_score (float): First repository score
    repo2_score (float): Second repository score
    income_score (float): Income evaluation score

    Returns:
    dict: Dictionary with optimized weights for each factor
    """
    import numpy as np

    # Base weights (must sum to 1.0)
    base_weights = {
        'github': 0.25,
        'leetcode': 0.25,
        'repo1': 0.15,
        'repo2': 0.15,
        'income': 0.20
    }

    # Convert scores to numpy array for calculations
    scores = np.array([github_score, leetcode_score, repo1_score, repo2_score, income_score])

    # If all scores are 0, return base weights
    if np.sum(scores) == 0:
        return base_weights

    # Normalize scores (this ensures sum of weights = 1.0)
    normalized_scores = scores / np.sum(scores)

    # Calculate confidence based on score distribution
    # Higher scores get more weight impact
    confidence = min(0.5, np.max(scores) / 100)

    # Create hybrid weights with adaptive balancing
    # Blend base weights with normalized scores based on confidence
    weights = {}
    keys = ['github', 'leetcode', 'repo1', 'repo2', 'income']

    for i, key in enumerate(keys):
        weights[key] = base_weights[key] * (1 - confidence) + normalized_scores[i] * confidence

    # Ensure weights sum to 1.0 (accounting for floating point errors)
    weight_sum = sum(weights.values())
    for key in weights:
        weights[key] = weights[key] / weight_sum

    # Ensure minimum weights (no factor should be completely ignored)
    min_weight = 0.05
    for key in weights:
        if weights[key] < min_weight:
            weights[key] = min_weight

    # Normalize again to ensure sum is 1.0
    weight_sum = sum(weights.values())
    for key in weights:
        weights[key] = weights[key] / weight_sum

    return weights


#-----------------------------------------------------------------------------------------------------------------
def main(github_username, leetcode_username, repo_url1, repo_url2, income_thousands, csv_path="developer_scores.csv", use_ai_weights=True):
    """
    Main function to analyze developer profiles and generate an AI score.

    Parameters:
    github_username (str): GitHub username to analyze
    leetcode_username (str): LeetCode username to analyze
    repo_url1 (str): URL of the first repository to evaluate
    repo_url2 (str): URL of the second repository to evaluate
    income_thousands (float): Annual income in thousands (INR)
    csv_path (str): Path to save the CSV file with results
    use_ai_weights (bool): Whether to use AI-optimized weights

    Returns:
    float: Final AI developer score out of 100
    """
    # Validate repository URLs
    for url in [repo_url1, repo_url2]:
        if not url.startswith(("http://", "https://")):
            print(f"Invalid repository URL: {url}")
            return None

    # Run GitHub analysis
    github_results = run_github_analysis(github_username)
    github_score = 0
    if github_results is not None and not github_results.empty:
        github_score = github_results.loc[0, 'GitHub Score']  # Directly access the first row


    # Run LeetCode analysis
    leetcode_results = run_leetcode_analysis(leetcode_username)
    leetcode_score = 0
    if leetcode_results is not None and not leetcode_results.empty:
        leetcode_score = leetcode_results.loc[0, 'leetcode_score']  # Directly access the first row


    # Validate repo ownership and evaluate repositories
    repo1_score = 0
    repo2_score = 0

    # Check if repo1 belongs to the user
    if validate_repo_ownership(repo_url1, github_username):
        repo1_score = evaluate_repository(repo_url1)

    # Check if repo2 belongs to the user
    if validate_repo_ownership(repo_url2, github_username):
        repo2_score = evaluate_repository(repo_url2)

    # Evaluate income
    income_score = evaluate_income(income_thousands)

    # Calculate AI score with dynamic weights
    if use_ai_weights:
        # Get AI-optimized weights based on developer profile
        weights = determine_ai_weights(github_score, leetcode_score, repo1_score, repo2_score, income_score)

        # Apply AI weights to calculate final score
        skill_score = (
            github_score * weights['github'] +
            leetcode_score * weights['leetcode'] +
            repo1_score * weights['repo1'] +
            repo2_score * weights['repo2'] +
            income_score * weights['income']
        )
    else:
        # Use default fixed weights
        skill_score = (
            github_score * 0.25 +
            leetcode_score * 0.25 +
            repo1_score * 0.15 +
            repo2_score * 0.15 +
            income_score * 0.20
        )

    # Store data in CSV

    data = {
        'name': [github_username],
        'github_score': [github_score],
        'leetcode_score': [leetcode_score],
        'repo1_score': [repo1_score],
        'repo2_score': [repo2_score],
        'income': [income_thousands],
        'final_score': [skill_score]
    }

    # Add weights to data if using AI weights
    if use_ai_weights:
        data.update({
            'github_weight': [weights['github']],
            'leetcode_weight': [weights['leetcode']],
            'repo1_weight': [weights['repo1']],
            'repo2_weight': [weights['repo2']],
            'income_weight': [weights['income']]
        })

    df = pd.DataFrame(data)

    # Check if file exists to determine if we need to write headers
    file_exists = os.path.isfile(csv_path)

    # Write to CSV (append if file exists, create new with headers if not)
    df.to_csv(csv_path, mode='a', header=not file_exists, index=False)

    return skill_score

import sys 

if __name__ == "__main__":
    if len(sys.argv) != 6:
        print("Error: Invalid arguments", file=sys.stderr)
        sys.exit(1)
    
    username, leetcode, repo1, repo2, income = sys.argv[1:6]
    
    try:
        income = float(income)  # Ensure income is a number
        score = main(username, leetcode, repo1, repo2, income, use_ai_weights=True)
        print("python:",score)  #  Print only the numeric score (No extra logs!)
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)